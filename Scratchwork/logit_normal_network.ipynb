{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output means: tensor([[ 0.9186,  0.9832, -1.5293, -0.3186, -0.6826],\n",
      "        [ 0.9125,  0.6633, -1.8544, -0.8576, -0.7011]], grad_fn=<AddBackward0>)\n",
      "Output variances: tensor([[43.2472, 48.7017, 46.1232, 42.9346, 49.7729],\n",
      "        [42.1023, 47.7009, 43.7699, 43.6115, 50.1957]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BayesFactorLayer(nn.Module):\n",
    "    def __init__(self, num_input_neurons, num_output_neurons):\n",
    "        super(BayesFactorLayer, self).__init__()\n",
    "        self.num_input_neurons = num_input_neurons\n",
    "        self.num_output_neurons = num_output_neurons\n",
    "        \n",
    "        # Initialize parameters for the layer\n",
    "        self.mu_1 = nn.Parameter(torch.randn(num_output_neurons, num_input_neurons))\n",
    "        self.mu_2 = nn.Parameter(torch.randn(num_output_neurons, num_input_neurons))\n",
    "        self.var_1 = nn.Parameter(torch.ones(num_output_neurons, num_input_neurons))\n",
    "        self.var_2 = nn.Parameter(torch.ones(num_output_neurons, num_input_neurons))\n",
    "        \n",
    "        # Initialize biases for the output neurons\n",
    "        self.bias_mean = nn.Parameter(torch.zeros(1, num_output_neurons))\n",
    "        # self.bias_var = nn.Parameter(torch.ones(1, num_output_neurons))\n",
    "    \n",
    "    def forward(self, mu_x, var_x):\n",
    "        var_x_expanded = var_x.unsqueeze(1)\n",
    "        mu_x_expanded = mu_x.unsqueeze(1)\n",
    "        \n",
    "        v1x, v2x = self.var_1 + var_x_expanded, self.var_2 + var_x_expanded\n",
    "        s41, s42 = torch.square(self.var_1), torch.square(self.var_2)\n",
    "        emu1, emu2, emux = torch.exp(self.mu_1), torch.exp(self.mu_2), torch.exp(mu_x_expanded)\n",
    "        e1x, e2x = emu1 * emux, emu2 * emux\n",
    "        e1p1, e2p1, e1xp1, e2xp1 = emu1 + 1, emu2 + 1, e1x + 1, e2x + 1\n",
    "        e1op1, e2op1, e1xop1, e2xop1 = emu1 / e1p1, emu2 / e2p1, e1x / e1xp1, e2x / e2xp1\n",
    "        e1op12, e2op12, e1xop12, e2xop12 = e1op1 / e1p1, e2op1 / e2p1, e1xop1 / e1xp1, e2xop1 / e2xp1\n",
    "        \n",
    "        # Mean of log Bayes factor\n",
    "        mu_out = torch.log(e2p1 * e1xp1 / (e1p1 * e2xp1))\n",
    "        mu_out += (self.var_2 * emu2 / (e2p1 ** 2) + (v1x) * e1x / (e1xp1 ** 2)) / 2\n",
    "        mu_out -= (self.var_1 * emu1 / (e1p1 ** 2) + (v2x) * e2x / (e2xp1 ** 2)) / 2\n",
    "        \n",
    "        # Variance of log Bayes factor\n",
    "        var_out = self.var_1 * torch.square(e1op1) + self.var_2 * torch.square(e2op1) + v1x * torch.square(e1xop1) + v2x * torch.square(e2xop1)\n",
    "        var_out += (s41 * torch.square(e1op12) + s42 * torch.square(e2op12)) / 2\n",
    "        var_out += (torch.square(v1x * e1xop12) + torch.square(v2x * e2xop12)) / 2\n",
    "        var_out -= 2 * (self.var_1 * e1xop1 * e1op1 + self.var_2 * e2xop1 * e2op1 + var_x_expanded * e1xop1 * e2xop1)\n",
    "        var_out -= s41 * e1xop12 * e1op12 + s42 * e2xop12 * e2op12 + torch.square(var_x_expanded) * e1xop12 * e2xop12\n",
    "        \n",
    "        # Sum the log Bayes factors and add bias\n",
    "        mu_out_sum = mu_out.sum(dim=-1) + self.bias_mean\n",
    "        var_out_sum = var_out.sum(dim=-1)\n",
    "        # var_out_sum += self.bias_var\n",
    "        \n",
    "        return mu_out_sum, var_out_sum\n",
    "\n",
    "class BayesFactorNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(BayesFactorNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.bias_mean = nn.Parameter(torch.zeros(1, output_size))\n",
    "        self.bias_var = nn.Parameter(torch.ones(1, output_size))\n",
    "\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(BayesFactorLayer(prev_size, hidden_size))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.output_layer = BayesFactorLayer(prev_size, output_size)\n",
    "    \n",
    "    def forward(self, mu_x, var_x):\n",
    "        for layer in self.layers:\n",
    "            mu_x, var_x = layer(mu_x, var_x)\n",
    "        \n",
    "        mu_out, var_out = self.output_layer(mu_x, var_x)\n",
    "        mu_out += self.bias_mean\n",
    "        var_out += self.bias_var\n",
    "        return mu_out, var_out\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10  # Number of input neurons\n",
    "hidden_sizes = [20, 30]  # Sizes of hidden layers\n",
    "output_size = 5  # Number of output neurons\n",
    "batch_size = 2\n",
    "\n",
    "# Create the network\n",
    "model = BayesFactorNetwork(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Example input: mean and variance of input neurons\n",
    "mu_x = torch.randn(batch_size, input_size)\n",
    "var_x = torch.ones(batch_size, input_size)\n",
    "\n",
    "# Forward pass\n",
    "mu_out, var_out = model(mu_x, var_x)\n",
    "print(\"Output means:\", mu_out)\n",
    "print(\"Output variances:\", var_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
