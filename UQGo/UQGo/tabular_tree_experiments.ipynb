{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pettingzoo.classic import go_v5\n",
    "from SMPyBandits import Policies\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixedMOSSEcomputeAllIndex(self):\n",
    "    \"\"\" Compute the current indexes for all arms, in a vectorized manner.\"\"\"\n",
    "    pulls_of_suboptimal_arms = np.sum(self.pulls[self.pulls < np.sqrt(self.t)])\n",
    "    if pulls_of_suboptimal_arms > 0:\n",
    "        indexes = (self.rewards / self.pulls) + np.sqrt(0.5 * np.maximum(0, np.log(self.t / pulls_of_suboptimal_arms)) / self.pulls)\n",
    "    else:\n",
    "        indexes = (self.rewards / self.pulls) + np.sqrt(0.5 * np.maximum(0, np.log(self.t / (self.nbArms * self.pulls))) / self.pulls)\n",
    "    # indexes[self.pulls < 1] = float('+inf')\n",
    "    self.index[:] = indexes\n",
    "Policies.MOSSExperimental.computeAllIndex = fixedMOSSEcomputeAllIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n",
      "pulls:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "rewards:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "regret:0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(true_probs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# print(policy.index)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     action \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mchoice()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mbinomial(\u001b[39m1\u001b[39m,true_probs[action])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     policy\u001b[39m.\u001b[39mgetReward(action, reward)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/IndexPolicy.py:57\u001b[0m, in \u001b[0;36mIndexPolicy.choice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" In an index policy, choose an arm with maximal index (uniformly at random):\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[39m.. math:: A(t) \\sim U(\\arg\\max_{1 \\leq k \\leq K} I_k(t)).\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39m.. warning:: In almost all cases, there is a unique arm with maximal index, so we loose a lot of time with this generic code, but I couldn't find a way to be more efficient without loosing generality.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# I prefer to let this be another method, so child of IndexPolicy only needs to implement it (if they want, or just computeIndex)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomputeAllIndex()\n\u001b[1;32m     58\u001b[0m \u001b[39m# Uniform choice among the best arms\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/IndexPolicy.py:45\u001b[0m, in \u001b[0;36mIndexPolicy.computeAllIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Compute the current indexes for all arms. Possibly vectorized, by default it can *not* be vectorized automatically.\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m arm \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnbArms):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex[arm] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomputeIndex(arm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/Thompson.py:54\u001b[0m, in \u001b[0;36mThompson.computeIndex\u001b[0;34m(self, arm)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomputeIndex\u001b[39m(\u001b[39mself\u001b[39m, arm):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Compute the current index, at time t and after :math:`N_k(t)` pulls of arm k, giving :math:`S_k(t)` rewards of 1, by sampling from the Beta posterior:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    .. math::\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m        A(t) &\\sim U(\\arg\\max_{1 \\leq k \\leq K} I_k(t)),\\\\\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m        I_k(t) &\\sim \\mathrm{Beta}(1 + \\tilde{S_k}(t), 1 + \\tilde{N_k}(t) - \\tilde{S_k}(t)).\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior[arm]\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/Posterior/Beta.py:105\u001b[0m, in \u001b[0;36mBeta.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a random sample from the Beta posterior (using :func:`numpy.random.betavariate`).\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39m    - Used only by :class:`Thompson` Sampling and :class:`AdBandits` so far.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m betavariate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# meta_policy = Policies.UCB(len(policy_algorithm_list))\n",
    "# squares = np.zeros(len(policy_algorithm_list))\n",
    "regrets = []\n",
    "while True:\n",
    "    # meta_choice = meta_policy.choice()\n",
    "    # meta_choice = np.argmin(meta_policy.pulls)\n",
    "    # print(f'choice:{meta_choice}')\n",
    "    # policy_algorithm = Policies.Thompson()\n",
    "    num_arms = 10\n",
    "    num_fictitious_pulls = 1000\n",
    "    random_fictitious = False\n",
    "    if random_fictitious:\n",
    "        fictitious_pulls = np.random.dirichlet(num_arms*[1])*num_fictitious_pulls\n",
    "        fictitious_empirical_means = np.random.random(num_arms)\n",
    "    else:\n",
    "        fictitious_pulls = np.array(num_arms*[0])\n",
    "        fictitious_empirical_means = np.array(num_arms*[0])\n",
    "\n",
    "    # print(fictitious_pulls)\n",
    "\n",
    "    fictitious_rewards = fictitious_pulls * fictitious_empirical_means\n",
    "    # print(fictitious_rewards)\n",
    "    # policy = Policies.UCBoost_bq_h_lb(num_arms)\n",
    "    policy = Policies.Thompson(num_arms)\n",
    "    # policy = CPUCB(num_arms)\n",
    "    # policy = klUCB(num_arms)\n",
    "    policy.pulls = fictitious_pulls\n",
    "    # print(policy.pulls)\n",
    "    policy.rewards = fictitious_rewards\n",
    "    # true_probs = np.random.random(num_arms)\n",
    "    # true_probs = np.array([.5]* num_arms)\n",
    "    true_probs = scipy.stats.beta.rvs(50,50,size=num_arms)\n",
    "    # print(true_probs)\n",
    "\n",
    "    for i in range(10000):\n",
    "        # print(policy.index)\n",
    "        action = policy.choice()\n",
    "        reward = np.random.binomial(1,true_probs[action])\n",
    "        policy.getReward(action, reward)\n",
    "        # policy.pulls[action]+=1\n",
    "        # policy.rewards[action]+=reward\n",
    "        # print(action,reward)\n",
    "        # print(policy.pulls)\n",
    "        # policy.computeAllIndex()\n",
    "    # print(true_probs)\n",
    "    # print(true_probs.round(4))\n",
    "    print('pulls:')\n",
    "    print(policy.pulls)\n",
    "    print('rewards:')\n",
    "    print(policy.rewards)\n",
    "    # print(policy.rewards/policy.pulls)\n",
    "    regret = policy.pulls.sum()*true_probs.max()-(policy.pulls*true_probs).sum()\n",
    "    print(f'regret:{regret}')\n",
    "    regrets.append(regret)\n",
    "    # meta_policy.getReward(meta_choice, -regret)\n",
    "    # squares[meta_choice]+=regret**2\n",
    "    # print('meta mean rewards')\n",
    "    # print(meta_policy.rewards/(meta_policy.pulls+.0001))\n",
    "    # print('meta pulls:')\n",
    "    # score = -meta_policy.rewards/(meta_policy.pulls+.0001)\n",
    "    # stdev = np.sqrt(squares/(meta_policy.pulls+.0001)-score**2)\n",
    "    # mean_stdev = stdev/np.sqrt((meta_policy.pulls+.0001))\n",
    "    # print((pd.DataFrame({'Name':policy_algorithm_names, 'score': score, 'mean stdev': mean_stdev, 'stdev': stdev, 'pulls': meta_policy.pulls})).sort_values(by='score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.81907243849219\n",
      "212.99216058455568\n",
      "155.5071369422276\n",
      "226.02372515332354\n",
      "112.98389512624362\n",
      "77.8678887746537\n",
      "165.4348185652443\n",
      "137.17026163220817\n",
      "137.5214684578341\n",
      "131.85439741592472\n",
      "224.84802831556954\n",
      "156.6382402878089\n",
      "115.83497346140484\n",
      "137.0102733332651\n",
      "120.0689006283892\n",
      "118.47564883858922\n",
      "214.9011884654019\n",
      "96.18527248110422\n",
      "109.14179624815642\n",
      "118.89641899332582\n",
      "90.03221823513559\n",
      "92.87749395217088\n",
      "165.656132201314\n",
      "119.47180127695628\n",
      "453.89094477441085\n",
      "126.169174407215\n",
      "110.8916266592596\n",
      "138.91297575320732\n",
      "114.80461304278379\n",
      "214.1414715846904\n",
      "110.61614916055896\n",
      "118.58638211439211\n",
      "248.28402352634566\n",
      "63.38029769356581\n",
      "107.86827351452484\n",
      "86.5166334825526\n",
      "154.9338270693661\n",
      "131.4761117093749\n",
      "168.44415369633862\n",
      "154.00863774748086\n",
      "79.09273311523793\n",
      "200.36206920627865\n",
      "196.4283286172522\n",
      "119.04908200088539\n",
      "119.12001136695199\n",
      "125.95675864578334\n",
      "252.7985189553401\n",
      "411.7599108391132\n",
      "137.8327677820589\n",
      "130.0250318423341\n",
      "141.73916375768022\n",
      "153.4181200392677\n",
      "245.07515246259118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     action \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mchoice()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mbinomial(\u001b[39m1\u001b[39m,true_probs[action])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pulls[action]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/IndexPolicy.py:57\u001b[0m, in \u001b[0;36mIndexPolicy.choice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" In an index policy, choose an arm with maximal index (uniformly at random):\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[39m.. math:: A(t) \\sim U(\\arg\\max_{1 \\leq k \\leq K} I_k(t)).\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39m.. warning:: In almost all cases, there is a unique arm with maximal index, so we loose a lot of time with this generic code, but I couldn't find a way to be more efficient without loosing generality.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# I prefer to let this be another method, so child of IndexPolicy only needs to implement it (if they want, or just computeIndex)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomputeAllIndex()\n\u001b[1;32m     58\u001b[0m \u001b[39m# Uniform choice among the best arms\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/IndexPolicy.py:45\u001b[0m, in \u001b[0;36mIndexPolicy.computeAllIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Compute the current indexes for all arms. Possibly vectorized, by default it can *not* be vectorized automatically.\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m arm \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnbArms):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex[arm] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomputeIndex(arm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/Thompson.py:54\u001b[0m, in \u001b[0;36mThompson.computeIndex\u001b[0;34m(self, arm)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomputeIndex\u001b[39m(\u001b[39mself\u001b[39m, arm):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Compute the current index, at time t and after :math:`N_k(t)` pulls of arm k, giving :math:`S_k(t)` rewards of 1, by sampling from the Beta posterior:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    .. math::\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m        A(t) &\\sim U(\\arg\\max_{1 \\leq k \\leq K} I_k(t)),\\\\\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m        I_k(t) &\\sim \\mathrm{Beta}(1 + \\tilde{S_k}(t), 1 + \\tilde{N_k}(t) - \\tilde{S_k}(t)).\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior[arm]\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/SMPyBandits/Policies/Posterior/Beta.py:105\u001b[0m, in \u001b[0;36mBeta.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a random sample from the Beta posterior (using :func:`numpy.random.betavariate`).\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39m    - Used only by :class:`Thompson` Sampling and :class:`AdBandits` so far.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m betavariate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regrets = []\n",
    "for i in range(483):\n",
    "    true_probs = scipy.stats.beta.rvs(50,50,size=num_arms)\n",
    "    policy = Policies.Thompson(10,a=50, b=50)\n",
    "    pulls = np.zeros(10)\n",
    "    rewards = np.zeros(10)\n",
    "    for i in range(10000):\n",
    "        action = policy.choice()\n",
    "        reward = np.random.binomial(1,true_probs[action])\n",
    "        pulls[action]+=1\n",
    "        rewards[action]+=reward\n",
    "        policy.getReward(action, reward)\n",
    "    regret = np.max(true_probs)*10000-pulls.dot(true_probs)\n",
    "    print(regret)\n",
    "    regrets.append(regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = np.array(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.std()/np.sqrt(len(regrets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_list = [Policies.ProbabilityPursuit, Policies.EmpiricalMeans, Policies.UCB, Policies.UCBmin, Policies.UCBplus, Policies.UCBVtuned, Policies.MOSS, Policies.MOSSAnytime, Policies.MOSSExperimental, Policies.klUCB, Policies.klUCBloglog, Policies.klUCBPlus, Policies.klUCBswitchAnytime, Policies.DMED, Policies.DMEDPlus, Policies.AdBandits, Policies.LM_DSEE, Policies.BESA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_names = ['ProbabilityPursuit', 'EmpiricalMeans', 'UCB', 'UCBmin', 'UCBplus', 'UCBVtuned', 'MOSS', 'MOSSAnytime', 'MOSSExperimental', 'klUCB', 'klUCBloglog', 'klUCBPlus', 'klUCBswitchAnytime', 'DMED', 'DMEDPlus', 'AdBandits', 'LM_DSEE', 'BESA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame({'Name':policy_algorithm_names, 'score': -meta_policy.rewards/(meta_policy.pulls+.0001), 'pulls': meta_policy.pulls})).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(policy_algorithm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(policy_algorithm_list)):\n",
    "    try:\n",
    "        results.append(benchmark(policy_algorithm_list[i]))\n",
    "    except:\n",
    "        results.append(f'fail:{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Armed Bandit Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetically-Generated Tree-based Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, mean, game_length, depth, num_arms):\n",
    "        self.mean = mean\n",
    "        self.game_length = game_length\n",
    "        self.depth = depth\n",
    "        self.num_arms = num_arms\n",
    "        self.policy = Policies.klUCBPlus(num_arms)\n",
    "        self.fictitious_pulls = np.array(num_arms*[1])\n",
    "        self.fictitious_rewards = np.array(num_arms*[0])\n",
    "        self.policy.pulls = self.fictitious_pulls.copy()\n",
    "        self.policy.rewards = self.fictitious_rewards.copy()\n",
    "        if game_length == depth:\n",
    "            self.action_means = scipy.stats.bernoulli.rvs(self.mean, size=num_arms)\n",
    "        else:\n",
    "            self.action_means = scipy.stats.beta.rvs(self.mean*9+.001,(1-self.mean)*9+.001,size=num_arms)\n",
    "            # self.action_means = self.action_means*self.mean/self.action_means.mean()\n",
    "        self.next_nodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'a/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0\\r\\nAccept: */*\\r\\nAccept-Languag']\n",
      "Bad pipe message: %s [b' en-US,en;q=0.5\\r\\nAccept-Encoding: gzip, deflate, br\\r\\nPrefer: safe\\r\\nAccess-Control-Request-Method: GE']\n",
      "Bad pipe message: %s [b'\\nAccess-Control-Request-Headers: x-xsrftoken\\r\\nOrigin: https://api-e7d4574d.duosecur', b'y.com\\r\\nConnection: keep-alive\\r\\nSec-Fetch-Dest: empty\\r\\nSec-Fetch-Mode: cors\\r\\nSec-Fetch-Site: cross-site\\r\\n']\n"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "root_node = Node(mean=.5, game_length = size, depth = 1, num_arms = size)\n",
    "writer = SummaryWriter()\n",
    "reward_list = []\n",
    "big_number = 1e10\n",
    "win_condition = 100\n",
    "i = 0\n",
    "while True:\n",
    "    node=root_node\n",
    "    node_action_list = []\n",
    "    while True:\n",
    "        action = node.policy.choice()\n",
    "        node_action_list.append((node,action))\n",
    "        if node.depth == node.game_length:\n",
    "            reward = node.action_means[action]\n",
    "            writer.add_scalar('Reward', reward, str(i))\n",
    "            reward_list.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            node = node.next_nodes.setdefault(action, Node(mean=node.action_means[action], game_length=node.game_length, depth=node.depth+1, num_arms=node.num_arms))\n",
    "    node_solved = False\n",
    "    for node, action in reversed(node_action_list):\n",
    "        if node_solved !=0:\n",
    "            node.policy.pulls[action] = -node_solved * big_number\n",
    "            node.fictitious_rewards[action] \n",
    "            node.policy.pulls[action] +=1\n",
    "        else:\n",
    "            node.policy.getReward(action, (reward if node.depth%2 else 1-reward))\n",
    "        node_solved = 1 if any((node.policy.rewards==big_number)==(node.policy.pulls==big_number)) else (-1 if all(node.policy.rewards==-big_number) else 0)\n",
    "    if node_action_list[-1][0].depth % 2 == reward:\n",
    "        node_action_list[-2][0].policy.rewards[node_action_list[-2][1]] = -big_number\n",
    "    if len(reward_list)>win_condition and ((np.array(reward_list[-win_condition:])==1).all() or (np.array(reward_list[-win_condition:])==0).all()):\n",
    "        break\n",
    "    i +=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoNode():\n",
    "    def __init__(self, num_arms, legal_actions, policy_algorithm, player='black_0', state=None, fictitious_alphas = None, fictitious_betas = None):\n",
    "        self.num_arms = num_arms\n",
    "        self.legal_actions = legal_actions\n",
    "        self.player = player\n",
    "        self.state = state\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "        self.policy = self.policy_algorithm(num_arms)\n",
    "        if fictitious_alphas == None and fictitious_betas == None:\n",
    "            self.fictitious_pulls = np.ones(num_arms)\n",
    "            self.fictitious_rewards = np.ones(num_arms) / 2\n",
    "        else:\n",
    "            self.fictitious_pulls = fictitious_alphas + fictitious_betas\n",
    "            self.fictitious_rewards = fictitious_alphas\n",
    "        self.policy.pulls = self.fictitious_pulls.copy()\n",
    "        self.policy.rewards = self.fictitious_rewards.copy()\n",
    "        self.policy.t = self.policy.pulls.sum()\n",
    "        self.next_nodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(policy_algorithm):\n",
    "    env = go_v5.env(board_size = 3, komi = 3.5)\n",
    "    env.reset(seed=42)\n",
    "    root_node = GoNode(num_arms = env.last()[0]['action_mask'].sum(), legal_actions = env.last()[0]['action_mask'].nonzero()[0], policy_algorithm=policy_algorithm)\n",
    "    winner_list = []\n",
    "    unique_nodes = 0\n",
    "    big_number = 1e10\n",
    "    win_condition = 100\n",
    "    i = 0\n",
    "    while True:\n",
    "        node = root_node\n",
    "        node_action_list = []\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        mask = observation[\"action_mask\"]\n",
    "        for agent in env.agent_iter():\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                winner = reward if agent=='black_0' else -reward\n",
    "                winner = (winner + 1)/2\n",
    "                winner_list.append(winner)\n",
    "                break\n",
    "            policy_choice = node.policy.choice()\n",
    "            action = node.legal_actions[policy_choice]\n",
    "            node_action_list.append((node,policy_choice))\n",
    "            env.step(action)\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            mask = observation[\"action_mask\"]\n",
    "            if policy_choice not in node.next_nodes:\n",
    "                unique_nodes +=1\n",
    "            node = node.next_nodes.setdefault(policy_choice, GoNode(num_arms=np.count_nonzero(observation['action_mask']), legal_actions = mask.nonzero()[0], policy_algorithm=policy_algorithm, player = 'white_0' if node.player=='black_0' else 'black_0'))\n",
    "        env.close()\n",
    "\n",
    "        if {'black_0':1,'white_0':0}[node_action_list[-1][0].player] == winner:\n",
    "            node_solved=-1\n",
    "        else:\n",
    "            node_solved=1\n",
    "        for node, action in reversed(node_action_list):\n",
    "            if node_solved !=0:\n",
    "                # node.policy.rewards[action] = -node_solved * big_number\n",
    "                # node.policy.pulls[action] +=1\n",
    "                # node.policy.t +=1\n",
    "                node.policy.getReward(action, -node_solved*big_number)\n",
    "            else:\n",
    "                node.policy.getReward(action, (winner if node.player=='black_0' else 1-winner))\n",
    "            node_solved = 1 if any(node.policy.rewards==big_number) else (-1 if all(node.policy.rewards==-big_number) else 0)\n",
    "        \n",
    "        if len(winner_list)>win_condition and ((np.array(winner_list[-win_condition:])==1).all() or (np.array(winner_list[-win_condition:])==0).all()):\n",
    "            break\n",
    "        env.reset(seed=42)\n",
    "        i+=1\n",
    "    return unique_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klUCBPlus_benchmark_results = []\n",
    "MOSSExperimental_benchmark_results = []\n",
    "while True:\n",
    "    if len(klUCBPlus_benchmark_results) <= len(MOSSExperimental_benchmark_results):\n",
    "        result = benchmark(Policies.klUCBPlus)\n",
    "        klUCBPlus_benchmark_results.append(result)\n",
    "        print(f'klUCBPlus: {result}')\n",
    "    else:\n",
    "        result = benchmark(Policies.MOSSExperimental)\n",
    "        MOSSExperimental_benchmark_results.append(result)\n",
    "        print(f'MOSSExperimental: {result}')\n",
    "    print(len(klUCBPlus_benchmark_results), len(MOSSExperimental_benchmark_results))\n",
    "    # bins = np.linspace(min(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), max(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), 20)\n",
    "    # plt.hist(klUCBPlus_benchmark_results, bins,alpha=.5)\n",
    "    # plt.hist(MOSSExperimental_benchmark_results, bins, alpha=.5)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), max(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), 40)\n",
    "plt.hist(klUCBPlus_benchmark_results, bins,alpha=.5)\n",
    "plt.hist(MOSSExperimental_benchmark_results, bins, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([1,1,1,0,0])\n",
    "d = np.array([0,0,1,1,1])\n",
    "(c==1)==(d==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(node):\n",
    "    states = [node.state]\n",
    "    num_arms = len(root_node.state[0].flatten())+1\n",
    "    a = np.zeros(num_arms)\n",
    "    a[node.legal_actions] = node.policy.rewards\n",
    "    alphas = [a]\n",
    "    p = np.zeros(num_arms)\n",
    "    p[node.legal_actions] = node.policy.pulls\n",
    "    betas = [p-a]\n",
    "    for next_node in node.next_nodes.values():\n",
    "        next_states, next_alphas, next_betas = pull_data(next_node)\n",
    "        states += next_states\n",
    "        alphas += next_alphas\n",
    "        betas += next_betas\n",
    "    return states, alphas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "31\n",
      "52\n",
      "71\n",
      "90\n",
      "117\n",
      "146\n",
      "182\n",
      "229\n",
      "253\n",
      "301\n",
      "327\n",
      "361\n",
      "388\n",
      "461\n",
      "550\n",
      "625\n",
      "677\n",
      "701\n",
      "738\n",
      "781\n",
      "820\n",
      "848\n",
      "882\n",
      "906\n",
      "929\n",
      "1013\n",
      "1041\n",
      "1078\n",
      "1117\n",
      "1160\n",
      "1165\n",
      "1192\n",
      "1227\n",
      "1260\n",
      "1284\n",
      "1316\n",
      "1367\n",
      "1398\n",
      "1433\n",
      "1479\n",
      "1504\n",
      "1550\n",
      "1632\n",
      "1676\n",
      "1685\n",
      "1713\n",
      "1741\n",
      "1788\n",
      "1837\n",
      "1871\n",
      "1899\n",
      "1931\n",
      "1942\n",
      "1992\n",
      "2009\n",
      "2061\n",
      "2081\n",
      "2113\n",
      "2139\n",
      "2207\n",
      "2225\n",
      "2269\n",
      "2292\n",
      "2321\n",
      "2357\n",
      "2383\n",
      "2407\n",
      "2436\n",
      "2467\n",
      "2495\n",
      "2519\n",
      "2561\n",
      "2586\n",
      "2674\n",
      "2775\n",
      "2823\n",
      "2852\n",
      "2888\n",
      "2916\n",
      "2956\n",
      "2983\n",
      "3002\n",
      "3052\n",
      "3096\n",
      "3134\n",
      "3170\n",
      "3266\n",
      "3296\n",
      "3305\n",
      "3360\n",
      "3409\n",
      "3449\n",
      "3475\n",
      "3500\n",
      "3532\n",
      "3562\n",
      "3608\n",
      "3633\n",
      "3664\n",
      "3678\n",
      "3744\n",
      "3794\n",
      "3820\n",
      "3864\n",
      "3894\n",
      "3969\n",
      "3998\n",
      "4046\n",
      "4097\n",
      "4120\n",
      "4168\n",
      "4226\n",
      "4251\n",
      "4289\n",
      "4302\n",
      "4337\n",
      "4364\n",
      "4419\n",
      "4430\n",
      "4447\n",
      "4515\n",
      "4542\n",
      "4593\n",
      "4616\n",
      "4665\n",
      "4720\n",
      "4749\n",
      "4809\n",
      "4838\n",
      "4877\n",
      "4915\n",
      "4943\n",
      "5021\n",
      "5061\n",
      "5089\n",
      "5113\n",
      "5146\n",
      "5177\n",
      "5216\n",
      "5263\n",
      "5294\n",
      "5319\n",
      "5360\n",
      "5386\n",
      "5411\n",
      "5495\n",
      "5528\n",
      "5549\n",
      "5580\n",
      "5618\n",
      "5641\n",
      "5669\n",
      "5701\n",
      "5737\n",
      "5777\n",
      "5825\n",
      "5863\n",
      "5893\n",
      "5929\n",
      "5981\n",
      "6024\n",
      "6073\n",
      "6106\n",
      "6134\n",
      "6184\n",
      "6207\n",
      "6253\n",
      "6294\n",
      "6331\n",
      "6358\n",
      "6378\n",
      "6397\n",
      "6419\n",
      "6456\n",
      "6475\n",
      "6502\n",
      "6532\n",
      "6558\n",
      "6610\n",
      "6650\n",
      "6681\n",
      "6822\n",
      "6865\n",
      "6898\n",
      "6923\n",
      "6950\n",
      "6977\n",
      "7008\n",
      "7050\n",
      "7085\n",
      "7115\n",
      "7127\n",
      "7202\n",
      "7229\n",
      "7257\n",
      "7299\n",
      "7330\n",
      "7343\n",
      "7361\n",
      "7422\n",
      "7448\n",
      "7485\n",
      "7510\n",
      "7535\n",
      "7579\n",
      "7607\n",
      "7626\n",
      "7665\n",
      "7690\n",
      "7742\n",
      "7768\n",
      "7793\n",
      "7834\n",
      "7864\n",
      "7886\n",
      "7956\n",
      "7971\n",
      "7999\n",
      "8006\n",
      "8048\n",
      "8078\n",
      "8095\n",
      "8131\n",
      "8150\n",
      "8185\n",
      "8210\n",
      "8249\n",
      "8274\n",
      "8316\n",
      "8391\n",
      "8428\n",
      "8466\n",
      "8512\n",
      "8541\n",
      "8559\n",
      "8578\n",
      "8604\n",
      "8635\n",
      "8660\n",
      "8690\n",
      "8712\n",
      "8731\n",
      "8772\n",
      "8815\n",
      "8853\n",
      "8875\n",
      "8894\n",
      "8928\n",
      "8950\n",
      "9011\n",
      "9048\n",
      "9067\n",
      "9122\n",
      "9146\n",
      "9288\n",
      "9304\n",
      "9345\n",
      "9347\n",
      "9353\n",
      "9427\n",
      "9450\n",
      "9473\n",
      "9501\n",
      "9541\n",
      "9570\n",
      "9601\n",
      "9629\n",
      "9675\n",
      "9706\n",
      "9744\n",
      "9745\n",
      "9767\n",
      "9791\n",
      "9836\n",
      "9872\n",
      "9917\n",
      "9928\n",
      "9961\n",
      "9996\n",
      "10043\n",
      "10089\n",
      "10129\n",
      "10165\n",
      "10191\n",
      "10218\n",
      "10252\n",
      "10277\n",
      "10303\n",
      "10341\n",
      "10397\n",
      "10423\n",
      "10450\n",
      "10472\n",
      "10513\n",
      "10532\n",
      "10560\n",
      "10598\n",
      "10645\n",
      "10676\n",
      "10717\n",
      "10741\n",
      "10782\n",
      "10813\n",
      "10853\n",
      "10885\n",
      "10927\n",
      "10969\n",
      "10992\n",
      "11019\n",
      "11050\n",
      "11075\n",
      "11115\n",
      "11132\n",
      "11161\n",
      "11192\n",
      "11217\n",
      "11259\n",
      "11286\n",
      "11316\n",
      "11362\n",
      "11411\n",
      "11439\n",
      "11466\n",
      "11494\n",
      "11516\n",
      "11539\n",
      "11573\n",
      "11590\n",
      "11618\n",
      "11648\n",
      "11671\n",
      "11696\n",
      "11730\n",
      "11751\n",
      "11791\n",
      "11832\n",
      "11875\n",
      "11916\n",
      "11967\n",
      "12003\n",
      "12028\n",
      "12067\n",
      "12075\n",
      "12101\n",
      "12132\n",
      "12156\n",
      "12185\n",
      "12217\n",
      "12242\n",
      "12276\n",
      "12324\n",
      "12382\n",
      "12417\n",
      "12447\n",
      "12489\n",
      "12539\n",
      "12570\n",
      "12595\n",
      "12638\n",
      "12664\n",
      "12729\n",
      "12753\n",
      "12771\n",
      "12800\n",
      "12802\n",
      "12841\n",
      "12857\n",
      "12893\n",
      "12949\n",
      "12963\n",
      "12995\n",
      "13015\n",
      "13048\n",
      "13081\n",
      "13104\n",
      "13151\n",
      "13175\n",
      "13205\n",
      "13229\n",
      "13261\n",
      "13279\n",
      "13310\n",
      "13335\n",
      "13366\n",
      "13392\n",
      "13422\n",
      "13464\n",
      "13497\n",
      "13560\n",
      "13590\n",
      "13651\n",
      "13676\n",
      "13734\n",
      "13769\n",
      "13819\n",
      "13836\n",
      "13882\n",
      "13931\n",
      "13985\n",
      "14012\n",
      "14039\n",
      "14063\n",
      "14113\n",
      "14155\n",
      "14179\n",
      "14193\n",
      "14245\n",
      "14290\n",
      "14328\n",
      "14355\n",
      "14394\n",
      "14438\n",
      "14458\n",
      "14482\n",
      "14522\n",
      "14608\n",
      "14628\n",
      "14679\n",
      "14717\n",
      "14752\n",
      "14790\n",
      "14792\n",
      "14814\n",
      "14848\n",
      "14870\n",
      "14915\n",
      "14936\n",
      "14990\n",
      "15050\n",
      "15082\n",
      "15103\n",
      "15156\n",
      "15208\n",
      "15311\n",
      "15342\n",
      "15450\n",
      "15456\n",
      "15502\n",
      "15523\n",
      "15555\n",
      "15594\n",
      "15637\n",
      "15677\n",
      "15728\n",
      "15743\n",
      "15789\n",
      "15813\n",
      "15856\n",
      "15877\n",
      "15895\n",
      "15937\n",
      "16044\n",
      "16094\n",
      "16122\n",
      "16161\n",
      "16187\n",
      "16228\n",
      "16253\n",
      "16303\n",
      "16330\n",
      "16379\n",
      "16405\n",
      "16429\n",
      "16446\n",
      "16474\n",
      "16494\n",
      "16526\n",
      "16641\n",
      "16669\n",
      "16702\n",
      "16707\n",
      "16756\n",
      "16786\n",
      "16835\n",
      "16888\n",
      "16912\n",
      "16957\n",
      "16981\n",
      "17006\n",
      "17037\n",
      "17067\n",
      "17155\n",
      "17179\n",
      "17204\n",
      "17226\n",
      "17273\n",
      "17316\n",
      "17359\n",
      "17383\n",
      "17409\n",
      "17441\n",
      "17469\n",
      "17528\n",
      "17554\n",
      "17600\n",
      "17639\n",
      "17675\n",
      "17684\n",
      "17712\n",
      "17758\n",
      "17795\n",
      "17839\n",
      "17844\n",
      "17872\n",
      "17911\n",
      "17938\n",
      "17958\n",
      "18004\n",
      "18035\n",
      "18064\n",
      "18108\n",
      "18161\n",
      "18190\n",
      "18220\n",
      "18260\n",
      "18312\n",
      "18337\n",
      "18382\n",
      "18407\n",
      "18450\n",
      "18480\n",
      "18563\n",
      "18601\n",
      "18639\n",
      "18662\n",
      "18689\n",
      "18719\n",
      "18779\n",
      "18824\n",
      "18861\n",
      "18914\n",
      "18960\n",
      "18990\n",
      "19015\n",
      "19054\n",
      "19084\n",
      "19122\n",
      "19169\n",
      "19192\n",
      "19216\n",
      "19242\n",
      "19273\n",
      "19294\n",
      "19321\n",
      "19373\n",
      "19401\n",
      "19454\n",
      "19480\n",
      "19530\n",
      "19554\n",
      "19588\n",
      "19606\n",
      "19644\n",
      "19676\n",
      "19699\n",
      "19731\n",
      "19755\n",
      "19783\n",
      "19816\n",
      "19943\n",
      "19969\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Net.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb Cell 27\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#X35sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m states, target_alphas, target_betas \u001b[39m=\u001b[39m pull_data(root_node)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#X35sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_network:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#X35sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     network \u001b[39m=\u001b[39m Net(\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#X35sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     using_network \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/philipnielsen/Documents/Code/UQGo/UQGo/tabular_tree_experiments.ipynb#X35sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m train(network, states, target_alphas, target_betas)\n",
      "\u001b[0;31mTypeError\u001b[0m: Net.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "env = go_v5.env(board_size = 5, komi = 3.5)\n",
    "policy_algorithm=Policies.MOSSExperimental\n",
    "env.reset(seed=42)\n",
    "last = env.last()\n",
    "root_node = GoNode(num_arms = last[0]['action_mask'].sum(), legal_actions = last[0]['action_mask'].nonzero()[0], policy_algorithm=policy_algorithm, state = last[0]['observation'].transpose((2,1,0)))\n",
    "winner_list = []\n",
    "big_number = 1e10\n",
    "win_condition = 100\n",
    "i = 0\n",
    "using_network=False\n",
    "states_per_training_batch = 20000\n",
    "num_unique_states_visited = 1\n",
    "# data collection and training loop\n",
    "while True:\n",
    "    # data collection and tree update loop\n",
    "    while num_unique_states_visited < states_per_training_batch:\n",
    "        print(num_unique_states_visited)\n",
    "        node = root_node\n",
    "        node_action_list = []\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        state = observation['observation'].transpose((2,1,0))\n",
    "        mask = observation[\"action_mask\"]\n",
    "        # single game playout loop\n",
    "        for agent in env.agent_iter():\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                winner = reward if agent=='black_0' else -reward\n",
    "                winner = (winner + 1)/2\n",
    "                winner_list.append(winner)\n",
    "                writer.add_scalar('Winner', winner, str(i))\n",
    "                break\n",
    "            policy_choice = node.policy.choice()\n",
    "            action = node.legal_actions[policy_choice]\n",
    "            node_action_list.append((node,policy_choice))\n",
    "            env.step(action)\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            state = observation['observation'].transpose((2,1,0))\n",
    "            mask = observation[\"action_mask\"]\n",
    "            num_arms = np.count_nonzero(mask)\n",
    "            if policy_choice in node.next_nodes:\n",
    "                node = node.next_nodes[policy_choice]\n",
    "            else:\n",
    "                fictitious_alphas, fictitious_betas = network(state) if using_network else (None, None)\n",
    "                node.next_nodes[policy_choice] = GoNode(num_arms=num_arms, \n",
    "                                                        legal_actions = mask.nonzero()[0], \n",
    "                                                        policy_algorithm=policy_algorithm if num_arms > 3 else Policies.klUCBPlus, \n",
    "                                                        player = 'white_0' if node.player=='black_0' else 'black_0', \n",
    "                                                        state=state,\n",
    "                                                        fictitious_alphas = fictitious_alphas,\n",
    "                                                        fictitious_betas = fictitious_betas)\n",
    "                num_unique_states_visited += 1\n",
    "                node = node.next_nodes[policy_choice]\n",
    "        env.close()\n",
    "\n",
    "        if {'black_0':1,'white_0':0}[node_action_list[-1][0].player] == winner:\n",
    "            node_solved=-1\n",
    "        else:\n",
    "            node_solved=1\n",
    "        # tree update loop\n",
    "        for node, action in reversed(node_action_list):\n",
    "            if node_solved !=0:\n",
    "                node.policy.rewards[action] = big_number if node_solved==-1 else .5\n",
    "                node.policy.pulls[action]=big_number\n",
    "                # node.policy.pulls[action] +=1\n",
    "            else:\n",
    "                node.policy.getReward(action, (winner if node.player=='black_0' else 1-winner))\n",
    "            node_solved = 1 if any(node.policy.rewards==big_number) else (-1 if (all(node.policy.rewards==.5) and all(node.policy.pulls==big_number)) else 0)\n",
    "        \n",
    "        # if len(winner_list)>win_condition and ((np.array(winner_list[-win_condition:])==1).all() or (np.array(winner_list[-win_condition:])==0).all()):\n",
    "        #     break\n",
    "        if any(root_node.policy.rewards==big_number):\n",
    "            break\n",
    "        env.reset(seed=42)\n",
    "\n",
    "    states, target_alphas, target_betas = pull_data(root_node)\n",
    "    if not using_network:\n",
    "        network = Net(5)\n",
    "        using_network = True\n",
    "    train(network, states, target_alphas, target_betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, target_alphas, target_betas = pull_data(root_node)\n",
    "target_alphas, target_betas = torch.FloatTensor(np.array(target_alphas)), torch.FloatTensor(np.array(target_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.5,  1.5,  3.5, 13.5,  4.5,  0.5,  1.5, 11.5, 11.5,  8.5,  1.5,\n",
       "         0.5,  0.5, 26.5,  5.5,  7.5, 16.5,  2.5, 39.5,  0.5,  8.5, 17.5,\n",
       "         8.5,  7.5,  5.5,  1.5]),\n",
       " array([ 5.5,  5.5,  9.5, 22.5, 10.5,  4.5,  5.5, 19.5, 19.5, 15.5,  5.5,\n",
       "         4.5,  4.5, 37.5, 11.5, 14.5, 25.5,  7.5, 51.5,  4.5, 16.5, 27.5,\n",
       "        15.5, 14.5, 11.5,  5.5]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_alphas[0], target_betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.FloatTensor(np.array(states))\n",
    "states.element_size()*states.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "            [nn.Conv2d(32, 32, 3, 1, padding='same') for i in range(2)] + \\\n",
    "            [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = F.relu(x)\n",
    "        out = torch.flatten(self.layers[-1](x), -2, -1)\n",
    "        passes = x[:,:2,:,:].mean(axis=(-1,-2))\n",
    "        out = torch.cat((out, passes.unsqueeze(-1)), axis=-1)\n",
    "        alphas = torch.exp(out[:,0,:])\n",
    "        betas=torch.exp(out[:,1,:])\n",
    "        return alphas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.next_nodes[0].policy.pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(alphas, betas, target_alphas, target_betas):\n",
    "    s=alphas+betas\n",
    "    p_hat = alphas/(alphas+betas)\n",
    "    return (target_alphas*(1-p_hat) + (target_alphas+target_betas)*p_hat**2 + alphas*betas/(s**2*(s+1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, states, target_alphas, target_betas):\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "    batch_size = 1024\n",
    "\n",
    "    train_target_alphas, test_target_alphas = torch.split(target_alphas, [int(len(target_alphas)*0.85), len(target_alphas) - int(len(target_alphas)*0.85)])\n",
    "    train_target_betas,  test_target_betas = torch.split(target_betas, [int(len(target_betas)*0.85), len(target_betas) - int(len(target_betas)*0.85)])\n",
    "    train_states, test_states = torch.split(states, [int(len(states)*0.85), len(states) - int(len(states)*0.85)])\n",
    "    \n",
    "    train_target_alpha_batches = torch.split(train_target_alphas, batch_size)\n",
    "    train_target_beta_batches = torch.split(train_target_betas, batch_size)\n",
    "    train_batches = torch.split(train_states, batch_size)\n",
    "        \n",
    "    for epoch in range(10):\n",
    "        print(f'epoch:{epoch}')\n",
    "        for i in range(len(train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "            aug_train_inpt, aug_train_target_alpha, aug_train_target_beta = augment(train_batches[i], train_target_alpha_batches[i], train_target_beta_batches[i])\n",
    "            alphas, betas = network(aug_train_inpt)\n",
    "            \n",
    "            # train_loss = loss_mse(alphas, betas, aug_train_target_alpha, aug_train_target_beta)\n",
    "            train_loss = loss4(alphas, betas, aug_train_target_alpha, aug_train_target_beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 40 == 0:\n",
    "                print(train_loss.item())\n",
    "        test_alphas, test_betas = network(test_states)\n",
    "        # val_loss = loss_mse(test_alphas, test_betas, test_target_alphas, test_target_betas)\n",
    "        val_loss = loss4(test_alphas, test_betas, test_target_alphas, test_target_betas)\n",
    "        print(f'val loss:{val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(5)\n",
    "train(net, torch.FloatTensor(states), torch.FloatTensor(target_alphas), torch.FloatTensor(target_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function to choose + cross-validate loss and iterations and network architecture other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Net, self).__init__()\n",
    "        if size == 1:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 2:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same') for _ in range(2)] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 3:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same') for _ in range(4)] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 4:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 64, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(64, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 16, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(16, 2, 3, 1, padding='same')])\n",
    "        elif size == 5:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 128, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(128, 64, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(64, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 16, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(16, 2, 3, 1, padding='same')])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = F.relu(x)\n",
    "        out = torch.flatten(self.layers[-1](x), -2, -1)\n",
    "        passes = x[:,:2,:,:].mean(axis=(-1,-2))\n",
    "        out = torch.cat((out, passes.unsqueeze(-1)), axis=-1)\n",
    "        alphas = torch.exp(out[:,0,:])\n",
    "        betas=torch.exp(out[:,1,:])\n",
    "        return alphas, betas\n",
    "    \n",
    "def augment(inpt, target_alphas, target_betas):\n",
    "    rotation = random.randrange(4)\n",
    "    flip = random.randrange(2)\n",
    "    alpha_board_targets , alpha_pass_targets = torch.split(target_alphas, [9,1], dim=-1)\n",
    "    beta_board_targets , beta_pass_targets = torch.split(target_betas, [9,1], dim=-1)\n",
    "    alpha_board_targets_reshaped = alpha_board_targets.view(-1, int(np.sqrt(alpha_board_targets.shape[-1])),int(np.sqrt(alpha_board_targets.shape[-1])))\n",
    "    beta_board_targets_reshaped = beta_board_targets.view(-1, int(np.sqrt(beta_board_targets.shape[-1])),int(np.sqrt(beta_board_targets.shape[-1])))\n",
    "    inpt = torch.rot90(inpt,rotation,[-2,-1])\n",
    "    alpha_board_targets_reshaped = torch.rot90(alpha_board_targets_reshaped,rotation,[-2,-1])\n",
    "    beta_board_targets_reshaped = torch.rot90(beta_board_targets_reshaped,rotation,[-2,-1])\n",
    "    if flip:\n",
    "        inpt = torch.flip(inpt, [-1])\n",
    "        alpha_board_targets_reshaped = torch.flip(alpha_board_targets_reshaped, [-1])\n",
    "        beta_board_targets_reshaped = torch.flip(beta_board_targets_reshaped, [-1])\n",
    "    alpha_board_targets = alpha_board_targets_reshaped.reshape(-1, alpha_board_targets.shape[-1])\n",
    "    beta_board_targets = beta_board_targets_reshaped.reshape(-1, beta_board_targets.shape[-1])\n",
    "    alpha_targets = torch.cat((alpha_board_targets, alpha_pass_targets),-1).contiguous()\n",
    "    beta_targets = torch.cat((beta_board_targets, beta_pass_targets),-1).contiguous()\n",
    "    return inpt, alpha_targets, beta_targets\n",
    "\n",
    "def loss1(target_alpha_batches, p_hat, s):\n",
    "    # return (target_beta_batches*p_hat**2 + target_alpha_batches*(1-p_hat)**2 + p_hat*(1-p_hat)/(s+1)*pulls).mean()\n",
    "    return (target_alpha_batches - target_alpha_batches*p_hat + p_hat*(1-p_hat)/(s+1)).mean()\n",
    "\n",
    "def loss2(p_hat, target_probs):\n",
    "    return ((p_hat-target_probs)**2).mean()\n",
    "\n",
    "def loss3(target_beta_batches, target_alpha_batches, p_hat):\n",
    "    return (target_beta_batches*p_hat**2 + target_alpha_batches*(1-p_hat)**2).mean()\n",
    "\n",
    "def loss4(alphas, betas, target_alphas, target_betas):\n",
    "    # return (target_alpha_batches*(torch.digamma(s)-torch.digamma(alphas))).mean()\n",
    "    return ((target_alphas + target_betas)*torch.digamma(alphas+betas)-target_alphas*torch.digamma(alphas)-target_betas*torch.digamma(betas)).mean()\n",
    "\n",
    "def tuning(network, loss_func):\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-5)\n",
    "\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_target_alphas, val_target_alphas, test_target_alphas = torch.split(target_alphas, [int(len(target_alphas)*0.7), int(len(target_alphas)*0.15), int(len(target_alphas)*0.15) + 1])\n",
    "    train_target_betas, val_target_betas, test_target_betas = torch.split(target_betas, [int(len(target_betas)*0.7), int(len(target_betas)*0.15), int(len(target_betas)*0.15) + 1])\n",
    "    train_states, val_states, test_states = torch.split(states, [int(len(states)*0.7), int(len(states)*0.15), int(len(states)*0.15) + 1])\n",
    "    \n",
    "    train_target_alpha_batches = torch.split(train_target_alphas, batch_size)\n",
    "    train_target_beta_batches = torch.split(train_target_betas, batch_size)\n",
    "    train_batches = torch.split(train_states, batch_size)\n",
    "    ma = 0\n",
    "        \n",
    "    for epoch in range(10):\n",
    "        print(f'epoch:{epoch}')\n",
    "        for i in range(len(train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "            aug_train_inpt, aug_train_target_alpha, aug_train_target_beta = augment(train_batches[i], train_target_alpha_batches[i], train_target_beta_batches[i])\n",
    "            alphas, betas = network(aug_train_inpt)\n",
    "\n",
    "            s = alphas+betas\n",
    "            p_hat = alphas/s\n",
    "            \n",
    "            pulls = aug_train_target_alpha + aug_train_target_beta\n",
    "            target_probs = aug_train_target_alpha/pulls\n",
    "            target_probs[target_probs != target_probs] = 0\n",
    "\n",
    "            if loss_func == loss1:\n",
    "                train_loss = loss1(aug_train_target_alpha, p_hat, s)\n",
    "            elif loss_func == loss2:\n",
    "                train_loss = loss2(p_hat, target_probs)\n",
    "            elif loss_func == loss3:\n",
    "                train_loss = loss3(aug_train_target_beta, aug_train_target_alpha, p_hat)\n",
    "            elif loss_func == loss4:\n",
    "                train_loss = loss4(aug_train_target_alpha, s, alphas)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 500 == 0:\n",
    "                print(train_loss.item())\n",
    "    \n",
    "    val_target_alpha_batches, val_target_beta_batches = torch.split(val_target_alphas, batch_size), torch.split(val_target_betas, batch_size)\n",
    "    val_batches = torch.split(val_states, batch_size)\n",
    "    val_batch_loss = []\n",
    "    for i in range(len(val_batches)):\n",
    "        optimizer.zero_grad()\n",
    "        alphas, betas = network(val_batches[i])\n",
    "\n",
    "        s = alphas+betas\n",
    "        p_hat = alphas/s\n",
    "        \n",
    "        pulls = val_target_alpha_batches[i] + val_target_beta_batches[i]\n",
    "        target_probs = val_target_alpha_batches[i]/pulls\n",
    "        target_probs[target_probs != target_probs] = 0\n",
    "\n",
    "        val_batch_loss += [loss1(val_target_alpha_batches[i], p_hat, s).item()]\n",
    "\n",
    "    return val_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "for architecture, loss_func in itertools.product(*[[Net(4), Net(5)],\n",
    "                                     [loss1, loss2, loss3, loss4]]):\n",
    "    val_losses.append(tuning(architecture, loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in val_losses:\n",
    "    print(sum(x)/352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i = 0\n",
    "best_error = np.Inf\n",
    "for i in range(len(val_losses)):\n",
    "    if np.mean(val_losses[i]) < best_error:\n",
    "        best_i = i\n",
    "        best_error = np.mean(val_losses[i])\n",
    "print(i)\n",
    "print(best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_target = torch.randn(64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_targets , pass_targets = torch.split(fake_target, [9,1], dim=-1)\n",
    "board_targets_reshaped = board_targets.view(-1, int(np.sqrt(board_targets.shape[-1])),int(np.sqrt(board_targets.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.sqrt(board_targets.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = torch.zeros(64,17,3,3)\n",
    "inpt[0,0,0,0]=1\n",
    "targets = torch.zeros(64,10)\n",
    "targets[0,0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment(inpt, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split(a,64)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network(torch.FloatTensor(root_node.next_nodes[4].state).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.next_nodes[4].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
